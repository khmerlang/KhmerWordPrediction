{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kh = pd.read_csv(\"data/data_kh.csv\", header=None)\n",
    "data_rom = pd.read_csv(\"data/data_rom.csv\", header=None)\n",
    "batch_size = 32  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 7154  # Number of samples to train on.\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 17911\n",
      "Number of unique input tokens: 76\n",
      "Number of unique output tokens: 70\n",
      "Max sequence length for inputs: 30\n",
      "Max sequence length for outputs: 47\n"
     ]
    }
   ],
   "source": [
    "for input_text in data_kh[0]:\n",
    "    input_text = str(input_text).strip()\n",
    "    input_texts.append(input_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "for target_text in data_rom[0]:\n",
    "    target_text = '\\t' + str(target_text).strip() + '\\n'\n",
    "    target_texts.append(target_text)\n",
    "    for char in str(target_text):\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ែ': 0, 'ប': 1, 'ល': 2, 'ថ': 3, 'ជ': 4, 'ឲ': 5, 'ា': 6, 'ិ': 7, 'ព': 8, 'រ': 9, 'េ': 10, 'ត': 11, 'ួ': 12, 'ញ': 13, 'ើ': 14, 'ទ': 15, 'ធ': 16, '័': 17, 'ឪ': 18, 'ំ': 19, 'ឍ': 20, '៎': 21, 'ឰ': 22, '៏': 23, 'ឥ': 24, 'ច': 25, 'ផ': 26, 'ង': 27, 'ៅ': 28, '់': 29, 'ឋ': 30, 'ឯ': 31, 'ឿ': 32, 'ី': 33, 'ឮ': 34, 'ោ': 35, 'ះ': 36, 'ណ': 37, 'វ': 38, 'ឆ': 39, 'ុ': 40, '៊': 41, '៉': 42, '៌': 43, 'ឫ': 44, 'ៃ': 45, 'គ': 46, 'ឦ': 47, 'អ': 48, 'ន': 49, 'ខ': 50, 'ឬ': 51, 'ឳ': 52, 'ឹ': 53, 'ហ': 54, 'យ': 55, 'ៀ': 56, '្': 57, 'ក': 58, 'ឡ': 59, 'ស': 60, 'ឱ': 61, 'ឌ': 62, 'ឭ': 63, 'ឈ': 64, 'ៈ': 65, '\\u200b': 66, 'ភ': 67, 'ដ': 68, '៍': 69, 'ឧ': 70, '?': 71, 'ូ': 72, 'ឺ': 73, 'ម': 74, 'ឃ': 75}\n",
      "{'်': 0, 'ရ': 1, 'F': 2, 'ဲ': 3, 'း': 4, 'ု': 5, 'g': 6, 'ာ': 7, 's': 8, 'o': 9, 't': 10, 'I': 11, 'd': 12, '?': 13, '\\n': 14, 'E': 15, 'ပ': 16, '\\t': 17, 'T': 18, 'ွ': 19, '့': 20, 'C': 21, 'p': 22, 'ဖ': 23, 'q': 24, 'm': 25, 'ေ': 26, 'e': 27, 'ျ': 28, 'S': 29, 'န': 30, 'b': 31, 'x': 32, 'L': 33, 'ီ': 34, 'ဂ': 35, '/': 36, 'ြ': 37, 'အ': 38, 'w': 39, 'K': 40, 'j': 41, '.': 42, 'ခ': 43, '1': 44, 'a': 45, 'ဟ': 46, 'င': 47, 'h': 48, 'စ': 49, '’': 50, 'က': 51, 'r': 52, 'u': 53, 'c': 54, 'y': 55, ' ': 56, 'M': 57, ':': 58, 'i': 59, 'l': 60, 'ယ': 61, '៍': 62, 'ိ': 63, 'ဘ': 64, 'n': 65, '8': 66, 'k': 67, 'မ': 68, 'v': 69}\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n",
      "[[[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]\n",
      "  [0. 0. 0. ... 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "print(input_token_index)\n",
    "print(target_token_index)\n",
    "\n",
    "print(encoder_input_data)\n",
    "print(decoder_input_data)\n",
    "print(decoder_target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    #encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n",
    "    \n",
    "# print(encoder_input_data[0])\n",
    "# print(decoder_input_data[0])\n",
    "# print(decoder_target_data[0])\n",
    "print(num_encoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14328 samples, validate on 3583 samples\n",
      "Epoch 1/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.5840 - acc: 0.8418 - val_loss: 0.5407 - val_acc: 0.8366\n",
      "Epoch 2/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.4432 - acc: 0.8664 - val_loss: 0.5123 - val_acc: 0.8445\n",
      "Epoch 3/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.4264 - acc: 0.8701 - val_loss: 0.4950 - val_acc: 0.8482\n",
      "Epoch 4/100\n",
      "14328/14328 [==============================] - 74s 5ms/step - loss: 0.4127 - acc: 0.8739 - val_loss: 0.4844 - val_acc: 0.8510\n",
      "Epoch 5/100\n",
      "14328/14328 [==============================] - 75s 5ms/step - loss: 0.4013 - acc: 0.8772 - val_loss: 0.4749 - val_acc: 0.8537\n",
      "Epoch 6/100\n",
      "14328/14328 [==============================] - 75s 5ms/step - loss: 0.3907 - acc: 0.8805 - val_loss: 0.4677 - val_acc: 0.8558\n",
      "Epoch 7/100\n",
      "14328/14328 [==============================] - 75s 5ms/step - loss: 0.3806 - acc: 0.8833 - val_loss: 0.4605 - val_acc: 0.8590\n",
      "Epoch 8/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.3715 - acc: 0.8862 - val_loss: 0.4563 - val_acc: 0.8609\n",
      "Epoch 9/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.3629 - acc: 0.8887 - val_loss: 0.4544 - val_acc: 0.8616\n",
      "Epoch 10/100\n",
      "14328/14328 [==============================] - 74s 5ms/step - loss: 0.3548 - acc: 0.8913 - val_loss: 0.4518 - val_acc: 0.8632\n",
      "Epoch 11/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.3472 - acc: 0.8936 - val_loss: 0.4496 - val_acc: 0.8637\n",
      "Epoch 12/100\n",
      "14328/14328 [==============================] - 90s 6ms/step - loss: 0.3399 - acc: 0.8954 - val_loss: 0.4491 - val_acc: 0.8641\n",
      "Epoch 13/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.3331 - acc: 0.8974 - val_loss: 0.4525 - val_acc: 0.8645\n",
      "Epoch 14/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.3269 - acc: 0.8991 - val_loss: 0.4529 - val_acc: 0.8638\n",
      "Epoch 15/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.3209 - acc: 0.9012 - val_loss: 0.4525 - val_acc: 0.8649\n",
      "Epoch 16/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.3152 - acc: 0.9028 - val_loss: 0.4601 - val_acc: 0.8639\n",
      "Epoch 17/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.3099 - acc: 0.9045 - val_loss: 0.4610 - val_acc: 0.8635\n",
      "Epoch 18/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.3048 - acc: 0.9057 - val_loss: 0.4625 - val_acc: 0.8649\n",
      "Epoch 19/100\n",
      "14328/14328 [==============================] - 88s 6ms/step - loss: 0.3000 - acc: 0.9073 - val_loss: 0.4674 - val_acc: 0.8639\n",
      "Epoch 20/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2954 - acc: 0.9086 - val_loss: 0.4692 - val_acc: 0.8642\n",
      "Epoch 21/100\n",
      "14328/14328 [==============================] - 86s 6ms/step - loss: 0.2913 - acc: 0.9099 - val_loss: 0.4711 - val_acc: 0.8646\n",
      "Epoch 22/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2875 - acc: 0.9111 - val_loss: 0.4745 - val_acc: 0.8643\n",
      "Epoch 23/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.2837 - acc: 0.9122 - val_loss: 0.4806 - val_acc: 0.8641\n",
      "Epoch 24/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.2803 - acc: 0.9129 - val_loss: 0.4809 - val_acc: 0.8642\n",
      "Epoch 25/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.2771 - acc: 0.9139 - val_loss: 0.4839 - val_acc: 0.8641\n",
      "Epoch 26/100\n",
      "14328/14328 [==============================] - 84s 6ms/step - loss: 0.2739 - acc: 0.9148 - val_loss: 0.4913 - val_acc: 0.8640\n",
      "Epoch 27/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2711 - acc: 0.9156 - val_loss: 0.4927 - val_acc: 0.8640\n",
      "Epoch 28/100\n",
      "14328/14328 [==============================] - 98s 7ms/step - loss: 0.2685 - acc: 0.9163 - val_loss: 0.4970 - val_acc: 0.8636\n",
      "Epoch 29/100\n",
      "14328/14328 [==============================] - 87s 6ms/step - loss: 0.2658 - acc: 0.9171 - val_loss: 0.5008 - val_acc: 0.8639\n",
      "Epoch 30/100\n",
      "14328/14328 [==============================] - 79s 6ms/step - loss: 0.2634 - acc: 0.9176 - val_loss: 0.5082 - val_acc: 0.8631\n",
      "Epoch 31/100\n",
      "14328/14328 [==============================] - 87s 6ms/step - loss: 0.2612 - acc: 0.9184 - val_loss: 0.5069 - val_acc: 0.8635\n",
      "Epoch 32/100\n",
      "14328/14328 [==============================] - 87s 6ms/step - loss: 0.2588 - acc: 0.9190 - val_loss: 0.5090 - val_acc: 0.8635\n",
      "Epoch 33/100\n",
      "14328/14328 [==============================] - 83s 6ms/step - loss: 0.2569 - acc: 0.9194 - val_loss: 0.5149 - val_acc: 0.8631\n",
      "Epoch 34/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.2549 - acc: 0.9199 - val_loss: 0.5179 - val_acc: 0.8630\n",
      "Epoch 35/100\n",
      "14328/14328 [==============================] - 83s 6ms/step - loss: 0.2530 - acc: 0.9205 - val_loss: 0.5246 - val_acc: 0.8624\n",
      "Epoch 36/100\n",
      "14328/14328 [==============================] - 79s 6ms/step - loss: 0.2513 - acc: 0.9209 - val_loss: 0.5276 - val_acc: 0.8628\n",
      "Epoch 37/100\n",
      "14328/14328 [==============================] - 79s 6ms/step - loss: 0.2496 - acc: 0.9213 - val_loss: 0.5290 - val_acc: 0.8623\n",
      "Epoch 38/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.2480 - acc: 0.9217 - val_loss: 0.5362 - val_acc: 0.8622\n",
      "Epoch 39/100\n",
      "14328/14328 [==============================] - 84s 6ms/step - loss: 0.2455 - acc: 0.9226 - val_loss: 0.5404 - val_acc: 0.8626\n",
      "Epoch 40/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.2449 - acc: 0.9226 - val_loss: 0.5415 - val_acc: 0.8622\n",
      "Epoch 41/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.2436 - acc: 0.9231 - val_loss: 0.5425 - val_acc: 0.8626\n",
      "Epoch 42/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2418 - acc: 0.9234 - val_loss: 0.5468 - val_acc: 0.8627\n",
      "Epoch 43/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2406 - acc: 0.9239 - val_loss: 0.5491 - val_acc: 0.8622\n",
      "Epoch 44/100\n",
      "14328/14328 [==============================] - 83s 6ms/step - loss: 0.2390 - acc: 0.9242 - val_loss: 0.5556 - val_acc: 0.8616\n",
      "Epoch 45/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2374 - acc: 0.9247 - val_loss: 0.5578 - val_acc: 0.8619\n",
      "Epoch 46/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.2361 - acc: 0.9248 - val_loss: 0.5563 - val_acc: 0.8619\n",
      "Epoch 47/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.2349 - acc: 0.9253 - val_loss: 0.5638 - val_acc: 0.8615\n",
      "Epoch 48/100\n",
      "14328/14328 [==============================] - 81s 6ms/step - loss: 0.2337 - acc: 0.9255 - val_loss: 0.5657 - val_acc: 0.8612\n",
      "Epoch 49/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2322 - acc: 0.9262 - val_loss: 0.5686 - val_acc: 0.8618\n",
      "Epoch 50/100\n",
      "14328/14328 [==============================] - 86s 6ms/step - loss: 0.2310 - acc: 0.9265 - val_loss: 0.5715 - val_acc: 0.8617\n",
      "Epoch 51/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.2298 - acc: 0.9269 - val_loss: 0.5755 - val_acc: 0.8614\n",
      "Epoch 52/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.2284 - acc: 0.9272 - val_loss: 0.5807 - val_acc: 0.8609\n",
      "Epoch 53/100\n",
      "14328/14328 [==============================] - 79s 5ms/step - loss: 0.2274 - acc: 0.9274 - val_loss: 0.5851 - val_acc: 0.8614\n",
      "Epoch 54/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.2259 - acc: 0.9280 - val_loss: 0.5890 - val_acc: 0.8601\n",
      "Epoch 55/100\n",
      "14328/14328 [==============================] - 79s 6ms/step - loss: 0.2247 - acc: 0.9285 - val_loss: 0.5932 - val_acc: 0.8607\n",
      "Epoch 56/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.2237 - acc: 0.9286 - val_loss: 0.5914 - val_acc: 0.8606\n",
      "Epoch 57/100\n",
      "14328/14328 [==============================] - 86s 6ms/step - loss: 0.2224 - acc: 0.9289 - val_loss: 0.5936 - val_acc: 0.8600\n",
      "Epoch 58/100\n",
      "14328/14328 [==============================] - 83s 6ms/step - loss: 0.2205 - acc: 0.9297 - val_loss: 0.5984 - val_acc: 0.8607\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2205 - acc: 0.9294 - val_loss: 0.6039 - val_acc: 0.8603\n",
      "Epoch 60/100\n",
      "14328/14328 [==============================] - 74s 5ms/step - loss: 0.2193 - acc: 0.9299 - val_loss: 0.6074 - val_acc: 0.8600\n",
      "Epoch 61/100\n",
      "14328/14328 [==============================] - 69s 5ms/step - loss: 0.2183 - acc: 0.9300 - val_loss: 0.6082 - val_acc: 0.8593\n",
      "Epoch 62/100\n",
      "14328/14328 [==============================] - 69s 5ms/step - loss: 0.2171 - acc: 0.9304 - val_loss: 0.6152 - val_acc: 0.8593\n",
      "Epoch 63/100\n",
      "14328/14328 [==============================] - 69s 5ms/step - loss: 0.2160 - acc: 0.9308 - val_loss: 0.6144 - val_acc: 0.8597\n",
      "Epoch 64/100\n",
      "14328/14328 [==============================] - 69s 5ms/step - loss: 0.2149 - acc: 0.9311 - val_loss: 0.6249 - val_acc: 0.8598\n",
      "Epoch 65/100\n",
      "14328/14328 [==============================] - 71s 5ms/step - loss: 0.2137 - acc: 0.9314 - val_loss: 0.6217 - val_acc: 0.8592\n",
      "Epoch 66/100\n",
      "14328/14328 [==============================] - 69s 5ms/step - loss: 0.2124 - acc: 0.9317 - val_loss: 0.6276 - val_acc: 0.8585\n",
      "Epoch 67/100\n",
      "14328/14328 [==============================] - 70s 5ms/step - loss: 0.2117 - acc: 0.9320 - val_loss: 0.6279 - val_acc: 0.8589\n",
      "Epoch 68/100\n",
      "14328/14328 [==============================] - 88s 6ms/step - loss: 0.2102 - acc: 0.9326 - val_loss: 0.6351 - val_acc: 0.8589\n",
      "Epoch 69/100\n",
      "14328/14328 [==============================] - 90s 6ms/step - loss: 0.2088 - acc: 0.9331 - val_loss: 0.6331 - val_acc: 0.8589\n",
      "Epoch 70/100\n",
      "14328/14328 [==============================] - 94s 7ms/step - loss: 0.2078 - acc: 0.9334 - val_loss: 0.6446 - val_acc: 0.8578\n",
      "Epoch 71/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2068 - acc: 0.9337 - val_loss: 0.6446 - val_acc: 0.8589\n",
      "Epoch 72/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.2057 - acc: 0.9340 - val_loss: 0.6516 - val_acc: 0.8583\n",
      "Epoch 73/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.2043 - acc: 0.9344 - val_loss: 0.6494 - val_acc: 0.8577\n",
      "Epoch 74/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2032 - acc: 0.9349 - val_loss: 0.6587 - val_acc: 0.8576\n",
      "Epoch 75/100\n",
      "14328/14328 [==============================] - 78s 5ms/step - loss: 0.2021 - acc: 0.9352 - val_loss: 0.6622 - val_acc: 0.8580\n",
      "Epoch 76/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.2011 - acc: 0.9353 - val_loss: 0.6637 - val_acc: 0.8575\n",
      "Epoch 77/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.1999 - acc: 0.9358 - val_loss: 0.6657 - val_acc: 0.8573\n",
      "Epoch 78/100\n",
      "14328/14328 [==============================] - 80s 6ms/step - loss: 0.1987 - acc: 0.9363 - val_loss: 0.6704 - val_acc: 0.8579\n",
      "Epoch 79/100\n",
      "14328/14328 [==============================] - 77s 5ms/step - loss: 0.1976 - acc: 0.9366 - val_loss: 0.6736 - val_acc: 0.8570\n",
      "Epoch 80/100\n",
      "14328/14328 [==============================] - 82s 6ms/step - loss: 0.1966 - acc: 0.9370 - val_loss: 0.6778 - val_acc: 0.8575\n",
      "Epoch 81/100\n",
      "14328/14328 [==============================] - 76s 5ms/step - loss: 0.1954 - acc: 0.9373 - val_loss: 0.6850 - val_acc: 0.8568\n",
      "Epoch 82/100\n",
      "14328/14328 [==============================] - 99s 7ms/step - loss: 0.1941 - acc: 0.9379 - val_loss: 0.6809 - val_acc: 0.8573\n",
      "Epoch 83/100\n",
      "14328/14328 [==============================] - 98s 7ms/step - loss: 0.1931 - acc: 0.9381 - val_loss: 0.6860 - val_acc: 0.8565\n",
      "Epoch 84/100\n",
      "14328/14328 [==============================] - 98s 7ms/step - loss: 0.1911 - acc: 0.9388 - val_loss: 0.6947 - val_acc: 0.8560\n",
      "Epoch 85/100\n",
      "14328/14328 [==============================] - 99s 7ms/step - loss: 0.1905 - acc: 0.9387 - val_loss: 0.6930 - val_acc: 0.8570\n",
      "Epoch 86/100\n",
      "14328/14328 [==============================] - 97s 7ms/step - loss: 0.1894 - acc: 0.9394 - val_loss: 0.6993 - val_acc: 0.8568\n",
      "Epoch 87/100\n",
      "14328/14328 [==============================] - 88s 6ms/step - loss: 0.1883 - acc: 0.9396 - val_loss: 0.6998 - val_acc: 0.8565\n",
      "Epoch 88/100\n",
      "14328/14328 [==============================] - 95s 7ms/step - loss: 0.1866 - acc: 0.9402 - val_loss: 0.7084 - val_acc: 0.8565\n",
      "Epoch 89/100\n",
      "14328/14328 [==============================] - 97s 7ms/step - loss: 0.1857 - acc: 0.9404 - val_loss: 0.7117 - val_acc: 0.8557\n",
      "Epoch 90/100\n",
      "14328/14328 [==============================] - 97s 7ms/step - loss: 0.1846 - acc: 0.9407 - val_loss: 0.7106 - val_acc: 0.8560\n",
      "Epoch 91/100\n",
      "14328/14328 [==============================] - 99s 7ms/step - loss: 0.1830 - acc: 0.9414 - val_loss: 0.7174 - val_acc: 0.8558\n",
      "Epoch 92/100\n",
      "14328/14328 [==============================] - 100s 7ms/step - loss: 0.1821 - acc: 0.9417 - val_loss: 0.7148 - val_acc: 0.8551\n",
      "Epoch 93/100\n",
      "14328/14328 [==============================] - 92s 6ms/step - loss: 0.1809 - acc: 0.9419 - val_loss: 0.7240 - val_acc: 0.8552\n",
      "Epoch 94/100\n",
      "14328/14328 [==============================] - 93s 6ms/step - loss: 0.1802 - acc: 0.9421 - val_loss: 0.7226 - val_acc: 0.8553\n",
      "Epoch 95/100\n",
      "14328/14328 [==============================] - 102s 7ms/step - loss: 0.1784 - acc: 0.9426 - val_loss: 0.7295 - val_acc: 0.8554\n",
      "Epoch 96/100\n",
      "14328/14328 [==============================] - 101s 7ms/step - loss: 0.1773 - acc: 0.9433 - val_loss: 0.7278 - val_acc: 0.8550\n",
      "Epoch 97/100\n",
      "14328/14328 [==============================] - 98s 7ms/step - loss: 0.1763 - acc: 0.9436 - val_loss: 0.7309 - val_acc: 0.8548\n",
      "Epoch 98/100\n",
      "14328/14328 [==============================] - 99s 7ms/step - loss: 0.1748 - acc: 0.9440 - val_loss: 0.7441 - val_acc: 0.8547\n",
      "Epoch 99/100\n",
      "14328/14328 [==============================] - 98s 7ms/step - loss: 0.1740 - acc: 0.9443 - val_loss: 0.7465 - val_acc: 0.8545\n",
      "Epoch 100/100\n",
      "14328/14328 [==============================] - 97s 7ms/step - loss: 0.1725 - acc: 0.9446 - val_loss: 0.7491 - val_acc: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56548299e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rathanak/anaconda3/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer lstm_4 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_3/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_3/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KH: បាន, Roman: \tban\n",
      ", predicted: cheatephoumeasa\n",
      "\n",
      "KH: ការ, Roman: \tkar\n",
      ", predicted: bangaeb ngkot\n",
      "\n",
      "KH: មាន, Roman: \tmean\n",
      ", predicted: angkhuosa\n",
      "\n",
      "KH: ជា, Roman: \tchea\n",
      ", predicted: cheaksteng\n",
      "\n",
      "KH: នៅ, Roman: \tnow\n",
      ", predicted: rotneakar\n",
      "\n",
      "KH: និង, Roman: \tning\n",
      ", predicted: pheang\n",
      "\n",
      "KH: ដែល, Roman: \tdel\n",
      ", predicted: dauch\n",
      "\n",
      "KH: ក្នុង, Roman: \tknong\n",
      ", predicted: kng\n",
      "\n",
      "KH: នេះ, Roman: \tnih\n",
      ", predicted: sami\n",
      "\n",
      "KH: ថា, Roman: \ttha\n",
      ", predicted: kaun\n",
      "\n",
      "KH: ទៅ, Roman: \ttow\n",
      ", predicted: kanhchungk\n",
      "\n",
      "KH: ពី, Roman: \tpi\n",
      ", predicted: phnek\n",
      "\n",
      "KH: មិន, Roman: \tmin\n",
      ", predicted: mean\n",
      "\n",
      "KH: របស់, Roman: \trobsa\n",
      ", predicted: brayeang\n",
      "\n",
      "KH: ទី, Roman: \tti\n",
      ", predicted: teang\n",
      "\n",
      "KH: មក, Roman: \tmk\n",
      ", predicted: saamrechchett\n",
      "\n",
      "KH: ឲ្យ, Roman: \taoy\n",
      ", predicted: cheab\n",
      "\n",
      "KH: នោះ, Roman: \tnoh\n",
      ", predicted: kraoyke bangaosa\n",
      "\n",
      "KH: ហើយ, Roman: \thaey\n",
      ", predicted: khosa\n",
      "\n",
      "KH: តែ, Roman: \tte\n",
      ", predicted: srabok\n",
      "\n",
      "KH: សម, Roman: \tsam\n",
      ", predicted: samont\n",
      "\n",
      "KH: លោក, Roman: \tlok\n",
      ", predicted: teangoasa\n",
      "\n",
      "KH: ដោយ, Roman: \tdaoy\n",
      ", predicted: touch\n",
      "\n",
      "KH: មួយ, Roman: \tmuoy\n",
      ", predicted: cheankraom\n",
      "\n",
      "KH: អ្នក, Roman: \tanak\n",
      ", predicted: kammovottho\n",
      "\n",
      "KH: យក, Roman: \tyk\n",
      ", predicted: ym\n",
      "\n",
      "KH: ថ្ងៃ, Roman: \tthngai\n",
      ", predicted: cheatetinn\n",
      "\n",
      "KH: នឹង, Roman: \tnung\n",
      ", predicted: khangokraom\n",
      "\n",
      "KH: ខែ, Roman: \tkhe\n",
      ", predicted: khang\n",
      "\n",
      "KH: ត្រូវ, Roman: \ttrauv\n",
      ", predicted: kraom maun\n",
      "\n",
      "KH: ធ្វើ, Roman: \tthveu\n",
      ", predicted: tam knea k\n",
      "\n",
      "KH: គេ, Roman: \tpuokke\n",
      ", predicted: krahamokk\n",
      "\n",
      "KH: ក៏, Roman: \tka\n",
      ", predicted: kha\n",
      "\n",
      "KH: រប, Roman: \tthoung\n",
      ", predicted: romlout\n",
      "\n",
      "KH: ខ្ញុំ, Roman: \tkhnhom\n",
      ", predicted: khnaoh\n",
      "\n",
      "KH: ឆ្នាំ, Roman: \tchhnam\n",
      ", predicted: kraom\n",
      "\n",
      "KH: មន, Roman: \tmn\n",
      ", predicted: bangaem\n",
      "\n",
      "KH: រក, Roman: \trk\n",
      ", predicted: bangaem\n",
      "\n",
      "KH: បង, Roman: \tbng\n",
      ", predicted: bangaem\n",
      "\n",
      "KH: ទេ, Roman: \tte\n",
      ", predicted: khleah\n",
      "\n",
      "KH: តាម, Roman: \ttam\n",
      ", predicted: thommosaphea\n",
      "\n",
      "KH: ជន, Roman: \tchn\n",
      ", predicted: rochnea sa msng\n",
      "\n",
      "KH: គឺ, Roman: \tku\n",
      ", predicted: kebre\n",
      "\n",
      "KH: ពេល, Roman: \tpel\n",
      ", predicted: kar saamrechchett\n",
      "\n",
      "KH: កម, Roman: \teb\n",
      ", predicted: bangaem\n",
      "\n",
      "KH: សង, Roman: \tsang\n",
      ", predicted: kamnoey\n",
      "\n",
      "KH: បក, Roman: \tbk\n",
      ", predicted: bangkoum\n",
      "\n",
      "KH: ខ្មែរ, Roman: \tkhmer\n",
      ", predicted: khmer\n",
      "\n",
      "KH: នក, Roman: \tnk\n",
      ", predicted: brachhuon\n",
      "\n",
      "KH: ដល់, Roman: \tdl\n",
      ", predicted: touk\n",
      "\n",
      "KH: ទាំង, Roman: \tteang\n",
      ", predicted: tosa\n",
      "\n",
      "KH: កម្ពុជា, Roman: \tkampouchea\n",
      ", predicted: kampoul\n",
      "\n",
      "KH: លើ, Roman: \tleu\n",
      ", predicted: thngai\n",
      "\n",
      "KH: បន, Roman: \tanak\n",
      ", predicted: cheab\n",
      "\n",
      "KH: ទៀត, Roman: \tphsaengtiet\n",
      ", predicted: tamlei\n",
      "\n",
      "KH: យើង, Roman: \tyeung\n",
      ", predicted: yeang\n",
      "\n",
      "KH: គ្នា, Roman: \tdauchaknea\n",
      ", predicted: kraom\n",
      "\n",
      "KH: រស, Roman: \trsa\n",
      ", predicted: rodthatout\n",
      "\n",
      "KH: នៃ, Roman: \tnei\n",
      ", predicted: rodthatoplea\n",
      "\n",
      "KH: ដើម្បី, Roman: \tdaembi\n",
      ", predicted: kaunobrosa\n",
      "\n",
      "KH: ប្រទេស, Roman: \tbratesa\n",
      ", predicted: bratesachitkheang\n",
      "\n",
      "KH: អាច, Roman: \tach\n",
      ", predicted: saampt\n",
      "\n",
      "KH: ដែរ, Roman: \tdauchaknea\n",
      ", predicted: dauchaknea\n",
      "\n",
      "KH: ដឹង, Roman: \tbandoeng\n",
      ", predicted: dauchaknea\n",
      "\n",
      "KH: គម, Roman: \tkm\n",
      ", predicted: smaeng\n",
      "\n",
      "KH: ឡើង, Roman: \tlaengleu\n",
      ", predicted: saambk khmaw\n",
      "\n",
      "KH: បើ, Roman: \tbae\n",
      ", predicted: bangaem\n",
      "\n",
      "KH: រម, Roman: \trm\n",
      ", predicted: karosmei\n",
      "\n",
      "KH: អន, Roman: \tan\n",
      ", predicted: khosa\n",
      "\n",
      "KH: ខ្លួន, Roman: \tkhluon\n",
      ", predicted: khluonokneng\n",
      "\n",
      "KH: ដាក់, Roman: \tdak\n",
      ", predicted: sangkhebrechch\n",
      "\n",
      "KH: ចូល, Roman: \tchaul\n",
      ", predicted: cheab\n",
      "\n",
      "KH: រហ, Roman: \trhek\n",
      ", predicted: athm\n",
      "\n",
      "KH: ឱ្យ, Roman: \taoy\n",
      ", predicted: kampk\n",
      "\n",
      "KH: ចេញ, Roman: \tchenh\n",
      ", predicted: koumrou\n",
      "\n",
      "KH: ដូច, Roman: \tdauch\n",
      ", predicted: thamopl khnat attouch\n",
      "\n",
      "KH: ណា, Roman: \tna na\n",
      ", predicted: saam\n",
      "\n",
      "KH: តម, Roman: \ttm\n",
      ", predicted: samner\n",
      "\n",
      "KH: យ៉ាង, Roman: \tyeang\n",
      ", predicted: brachea\n",
      "\n",
      "KH: រង, Roman: \trng\n",
      ", predicted: rng\n",
      "\n",
      "KH: ផង, Roman: \tphng der\n",
      ", predicted: kham\n",
      "\n",
      "KH: ទទួល, Roman: \tttuol\n",
      ", predicted: ttuol\n",
      "\n",
      "KH: ព្រះ, Roman: \tpreah\n",
      ", predicted: rong\n",
      "\n",
      "KH: គាត់, Roman: \tkeat\n",
      ", predicted: krout\n",
      "\n",
      "KH: ច្រើន, Roman: \tchraen\n",
      ", predicted: kampheng\n",
      "\n",
      "KH: វា, Roman: \tvea\n",
      ", predicted: banhchhot\n",
      "\n",
      "KH: វិញ, Roman: \tvinh\n",
      ", predicted: chomneanh\n",
      "\n",
      "KH: ស្រុក, Roman: \tsrok\n",
      ", predicted: phteahsambotr\n",
      "\n",
      "KH: ឯង, Roman: \ttemnakeng\n",
      ", predicted: sari\n",
      "\n",
      "KH: បត, Roman: \tbt\n",
      ", predicted: tomneaktomnng\n",
      "\n",
      "KH: ចំនួន, Roman: \tchamnuon\n",
      ", predicted: khangosdam\n",
      "\n",
      "KH: ចង, Roman: \tchng\n",
      ", predicted: cheate\n",
      "\n",
      "KH: និយាយ, Roman: \tniyeay\n",
      ", predicted: nea phteahaekea\n",
      "\n",
      "KH: ភាព, Roman: \tpheap\n",
      ", predicted: srak tukmeat\n",
      "\n",
      "KH: ឃើញ, Roman: \tkheunh\n",
      ", predicted: saam reay\n",
      "\n",
      "KH: ទង, Roman: \ttongophchet\n",
      ", predicted: tongoph\n",
      "\n",
      "KH: រយ, Roman: \tpheakory\n",
      ", predicted: rokm\n",
      "\n",
      "KH: ខាង, Roman: \tkhang\n",
      ", predicted: khang\n",
      "\n",
      "KH: ច្បាប់, Roman: \tchbab\n",
      ", predicted: khangke\n",
      "\n",
      "KH: នូវ, Roman: \tlau\n",
      ", predicted: theanea\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "#     if str(decoded_sentence).strip() != str(target_texts[seq_index]).strip():\n",
    "#         print('-')\n",
    "#         print('KH: ' + input_texts[seq_index] + ', Roman: ' + target_texts[seq_index] + ', predicted: ' + decoded_sentence)\n",
    "    print('KH: ' + input_texts[seq_index] + ', Roman: ' + target_texts[seq_index] + ', predicted: ' + decoded_sentence)\n",
    "#     print('Decoded sentence:', decoded_sentence)\n",
    "#     print('Actual sentence:', target_texts[seq_index])\n",
    "#     print(str(decoded_sentence).strip() == str(target_texts[seq_index]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
